---
# Model related configurations
model_family: llama2-7b-chat   # gemma-7b-it #  llama2-7b-chat  #  mistral-7b-instruct Qwen2-7B-Instruct  

# data related configurations
data_name: pistol_sample1 # pistol_sample1 # factual_forget # tofu_full
forget_edge: ['A_B'] # this should be a list # ['A_B'] ['author_1']
use_different_retain_dataset: false # if true, the retain dataset will be different from the forget dataset, otherwise it will be the same as the forget dataset  
different_retain_set_path: dataset/unlearning/factual_data.json # this is the path to the different retain dataset, only used if use_different_retain_dataset is true

#model_path: meta-llama/Llama-2-7b-chat-hf 
model_path: /nfs-share/fs604/PISTOL/models_finetune/pistol_sample1/llama2-7b-chat/20epochs_LoRA8_lr0.0001 # this dont need to changed unless you want to use pretrained model

# estimated net#
layer_modified: [17,16,18] # need to be list
coeff_list: [2.0,2.0,1.0] # need to be list
num_epochs: 50
use_harmful: true
use_unverified: false
lr: 0.01
positions: -1
n_train: 128
n_test: 100
n_val: 32
max_new_tokens: 64

# eval
eval_batch_size: 16
eval_generation_max_length: 512
eval_generation_max_new_tokens: 512

# save path
save_path: run_results/completions/${model_family}/reverse_attack/${data_name}
